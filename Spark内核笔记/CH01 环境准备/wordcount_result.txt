20/11/03 19:12:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/03 19:12:57 INFO SparkContext: Running Spark version 3.0.0
20/11/03 19:12:57 INFO ResourceUtils: ==============================================================
20/11/03 19:12:57 INFO ResourceUtils: Resources for spark.driver:

20/11/03 19:12:57 INFO ResourceUtils: ==============================================================
20/11/03 19:12:57 INFO SparkContext: Submitted application: Spark shell
20/11/03 19:12:57 INFO SecurityManager: Changing view acls to: YUANYIN
20/11/03 19:12:57 INFO SecurityManager: Changing modify acls to: YUANYIN
20/11/03 19:12:57 INFO SecurityManager: Changing view acls groups to:
20/11/03 19:12:57 INFO SecurityManager: Changing modify acls groups to:
20/11/03 19:12:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(YUANYIN); groups with view permissions: Set(); users  with modify permissions: Set(YUANYIN); groups with modify permissions: Set()
20/11/03 19:12:58 INFO Utils: Successfully started service 'sparkDriver' on port 64253.
20/11/03 19:12:58 INFO SparkEnv: Registering MapOutputTracker
20/11/03 19:12:58 INFO SparkEnv: Registering BlockManagerMaster
20/11/03 19:12:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/03 19:12:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/03 19:12:58 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/11/03 19:12:58 INFO DiskBlockManager: Created local directory at C:\Users\YUANYIN\AppData\Local\Temp\blockmgr-e6771575-bb50-421f-b789-7a38ae6233fa
20/11/03 19:12:58 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB
20/11/03 19:12:58 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/03 19:12:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/03 19:12:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://LAPTOP-0RVBFNM0:4040
20/11/03 19:12:58 INFO Executor: Starting executor ID driver on host LAPTOP-0RVBFNM0
20/11/03 19:12:58 INFO Executor: Using REPL class URI: spark://LAPTOP-0RVBFNM0:64253/classes
20/11/03 19:12:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64296.
20/11/03 19:12:58 INFO NettyBlockTransferService: Server created on LAPTOP-0RVBFNM0:64296
20/11/03 19:12:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/03 19:12:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, LAPTOP-0RVBFNM0, 64296, None)
20/11/03 19:12:58 INFO BlockManagerMasterEndpoint: Registering block manager LAPTOP-0RVBFNM0:64296 with 366.3 MiB RAM, BlockManagerId(driver, LAPTOP-0RVBFNM0, 64296, None)
20/11/03 19:12:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, LAPTOP-0RVBFNM0, 64296, None)
20/11/03 19:12:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, LAPTOP-0RVBFNM0, 64296, None)
20/11/03 19:12:59 INFO Main: Created Spark session
Spark context Web UI available at http://LAPTOP-0RVBFNM0:4040
Spark context available as 'sc' (master = local[*], app id = local-1604401978730).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.0
      /_/

Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_261)
Type in expressions to have them evaluated.
Type :help for more information.

scala> 20/11/03 19:13:09 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped


scala> val lines = sc.textFile("../CONTRIBUTING.md", 2)
20/11/03 19:14:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 289.7 KiB, free 366.0 MiB)
20/11/03 19:14:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.2 KiB, free 366.0 MiB)
20/11/03 19:14:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on LAPTOP-0RVBFNM0:64296 (size: 25.2 KiB, free: 366.3 MiB)
20/11/03 19:14:02 INFO SparkContext: Created broadcast 0 from textFile at <console>:24
lines: org.apache.spark.rdd.RDD[String] = ../CONTRIBUTING.md MapPartitionsRDD[1] at textFile at <console>:24

scala> val words = lines.flatMap(line => line.split(" "))
words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at flatMap at <console>:25

scala> val ones = words.map(w => (w, 1))
ones: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[3] at map at <console>:25

scala> val counts = ones.reduceByKey(_ + _)
20/11/03 19:15:07 INFO FileInputFormat: Total input files to process : 1
counts: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[4] at reduceByKey at <console>:25

scala> counts.foreach(println)
20/11/03 19:15:17 INFO SparkContext: Starting job: foreach at <console>:26
20/11/03 19:15:18 INFO DAGScheduler: Registering RDD 3 (map at <console>:25) as input to shuffle 0
20/11/03 19:15:18 INFO DAGScheduler: Got job 0 (foreach at <console>:26) with 2 output partitions
20/11/03 19:15:18 INFO DAGScheduler: Final stage: ResultStage 1 (foreach at <console>:26)
20/11/03 19:15:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/03 19:15:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/11/03 19:15:18 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at map at <console>:25), which has no missing parents
20/11/03 19:15:18 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.2 KiB, free 366.0 MiB)
20/11/03 19:15:18 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.0 KiB, free 366.0 MiB)
20/11/03 19:15:18 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on LAPTOP-0RVBFNM0:64296 (size: 4.0 KiB, free: 366.3 MiB)
20/11/03 19:15:18 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1200
20/11/03 19:15:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at map at <console>:25) (first 15 tasks are for partitions Vector(0, 1))
20/11/03 19:15:18 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
20/11/03 19:15:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, LAPTOP-0RVBFNM0, executor driver, partition 0, PROCESS_LOCAL, 7371 bytes)
20/11/03 19:15:18 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, LAPTOP-0RVBFNM0, executor driver, partition 1, PROCESS_LOCAL, 7371 bytes)
20/11/03 19:15:18 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
20/11/03 19:15:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/11/03 19:15:18 INFO HadoopRDD: Input split: file:/E:/Code/spark-3.0.0/CONTRIBUTING.md:498+499
20/11/03 19:15:18 INFO HadoopRDD: Input split: file:/E:/Code/spark-3.0.0/CONTRIBUTING.md:0+498
20/11/03 19:15:18 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1294 bytes result sent to driver
20/11/03 19:15:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1294 bytes result sent to driver
20/11/03 19:15:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 469 ms on LAPTOP-0RVBFNM0 (executor driver) (1/2)
20/11/03 19:15:18 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 458 ms on LAPTOP-0RVBFNM0 (executor driver) (2/2)
20/11/03 19:15:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
20/11/03 19:15:18 INFO DAGScheduler: ShuffleMapStage 0 (map at <console>:25) finished in 0.585 s
20/11/03 19:15:18 INFO DAGScheduler: looking for newly runnable stages
20/11/03 19:15:18 INFO DAGScheduler: running: Set()
20/11/03 19:15:18 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/11/03 19:15:18 INFO DAGScheduler: failed: Set()
20/11/03 19:15:18 INFO DAGScheduler: Submitting ResultStage 1 (ShuffledRDD[4] at reduceByKey at <console>:25), which has no missing parents
20/11/03 19:15:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.6 KiB, free 366.0 MiB)
20/11/03 19:15:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.6 KiB, free 366.0 MiB)
20/11/03 19:15:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on LAPTOP-0RVBFNM0:64296 (size: 2.6 KiB, free: 366.3 MiB)
20/11/03 19:15:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1200
20/11/03 19:15:18 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (ShuffledRDD[4] at reduceByKey at <console>:25) (first 15 tasks are for partitions Vector(0, 1))
20/11/03 19:15:18 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks
20/11/03 19:15:18 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, LAPTOP-0RVBFNM0, executor driver, partition 0, NODE_LOCAL, 7143 bytes)
20/11/03 19:15:18 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, LAPTOP-0RVBFNM0, executor driver, partition 1, NODE_LOCAL, 7143 bytes)
20/11/03 19:15:18 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
20/11/03 19:15:18 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)
20/11/03 19:15:18 INFO ShuffleBlockFetcherIterator: Getting 2 (555.0 B) non-empty blocks including 2 (555.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/11/03 19:15:18 INFO ShuffleBlockFetcherIterator: Getting 2 (638.0 B) non-empty blocks including 2 (638.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/11/03 19:15:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
20/11/03 19:15:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 19 ms
(lists,1)
(contribution,1)
(Spark,2)
(means,1)
(original,1)
(are,1)
(legal,1)
(this,2)
(related,1)
(review,1)
(warrant,1)
(via,1)
(ask,1)
(agree,1)
(affirm,1)
(is,1)
(before,1)
(under,2)
(code,,1)
(enough,1)
(Whether,1)
(opening,1)
(requests?,1)
(can,1)
(steps,1)
(have,1)
(In,1)
(Is,3)
(-,4)
(for,1)
(project](https://spark.apache.org/third-party-projects.html),1)
(?,1)
(motivated?,1)
(consider:,1)
(When,1)
(*Before,1)
(spend,1)
(explicitly,,1)
(Have,1)
(request*,,1)
(as,1)
(time,1)
(,3)
(searched,1)
(Contributing,1)
(required,1)
(copyrighted,1)
(JIRAs,1)
(proposed,1)
(your,1)
([third,1)
(the,11)
(submitting,1)
(ready,1)
(important,1)
(authority,1)
(email,,1)
(not,1)
(explained,1)
(existing,,1)
(new,1)
(alone,1)
(PR.,1)
(you,7)
(feature,1)
(project,1)
(any,1)
(that,5)
(so.,1)
(request,,1)
(guide](https://spark.apache.org/contributing.html).,1)
(a,4)
(##,1)
(or,2)
(other,1)
(contribute,1)
(party,1)
(community,1)
(open,2)
(creating,1)
(clearly,1)
(source,2)
(stand,1)
(pull,3)
(change,2)
(do,1)20/11/03 19:15:18 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1224 bytes result sent to driver

(work,2)
(to,7)
(state,1)
(material,2)20/11/03 19:15:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 174 ms on LAPTOP-0RVBFNM0 (executor driver) (1/2)

(license.,1)
(particular,,1)
(project's,2)
(license,3)
(by,1)
(It,1)
(and,5)
(being,1)
([Contributing,1)
(reviewing?,1)
20/11/03 19:15:18 INFO Executor: Finished task 1.0 in stage 1.0 (TID 3). 1224 bytes result sent to driver
20/11/03 19:15:18 INFO TaskSetManager: Finished task 1.0 in stage 1.0 (TID 3) in 211 ms on LAPTOP-0RVBFNM0 (executor driver) (2/2)
20/11/03 19:15:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
20/11/03 19:15:18 INFO DAGScheduler: ResultStage 1 (foreach at <console>:26) finished in 0.237 s
20/11/03 19:15:18 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/11/03 19:15:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/11/03 19:15:18 INFO DAGScheduler: Job 0 finished: foreach at <console>:26, took 1.053261 s
